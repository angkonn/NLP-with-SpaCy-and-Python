{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88acbb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2f0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee142b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Paul Newman was an American actor, but Paul Hollywood is a British TV Host. The name Paul is quite common.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"Paul [A-Z]\\w+\" #\\w+ requires at least one word character after the uppercase letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7740c851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 11), match='Paul Newman'>\n",
      "<re.Match object; span=(39, 53), match='Paul Hollywood'>\n"
     ]
    }
   ],
   "source": [
    "matches = re.finditer(pattern, text)\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ecc614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb39effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman PERSON\n",
      "Paul Hollywood PERSON\n"
     ]
    }
   ],
   "source": [
    "# Load a blank English spaCy model for tokenization\n",
    "# Purpose: Initializes a minimal spaCy pipeline for English with only a tokenizer.\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the input text to create a spaCy Doc object\n",
    "# Purpose: Converts the input string 'text' into a tokenized Doc object for further processing.\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities from the Doc and store as a list\n",
    "# Purpose: Attempts to retrieve named entities from the Doc, but since the model is blank, this is empty.\n",
    "# Explanation: doc.ents contains entities from an NER component, which is absent in a blank model, so doc.ents is an empty tuple. Converting to a list creates an empty original_ents.\n",
    "original_ents = list(doc.ents)\n",
    "\n",
    "# Initialize an empty list for storing multi-word token entities\n",
    "# Purpose: Creates a list to hold tuples of matched spans (start index, end index, text).\n",
    "mwt_ents = []\n",
    "\n",
    "# Find all non-overlapping matches of the regex pattern in the Doc's raw text\n",
    "# Purpose: Uses the regex pattern to identify matches in the text (e.g., names like \"Paul Smith\").\n",
    "for match in re.finditer(pattern, doc.text):\n",
    "    # Extract the start and end character indices of the regex match\n",
    "    # Purpose: Gets the character positions of the matched substring in the text.\n",
    "    start, end = match.span()\n",
    "    \n",
    "    # Convert character indices to a spaCy Span object aligned with token boundaries\n",
    "    # Purpose: Maps the character-based match to a token-based Span object.\n",
    "    span = doc.char_span(start, end)\n",
    "    \n",
    "    # Check if the Span is valid before storing\n",
    "    # Purpose: Ensures the Span aligns with token boundaries, skipping invalid matches.\n",
    "    if span is not None:\n",
    "        # Store the Span’s start token index, end token index, and text as a tuple\n",
    "        # Purpose: Saves the matched span’s details for further processing.\n",
    "        mwt_ents.append((span.start, span.end, span.text))\n",
    "\n",
    "# Convert matched spans into spaCy Span objects with a \"PERSON\" label\n",
    "# Purpose: Creates named entity Spans labeled as \"PERSON\" from the matched spans.\n",
    "for ent in mwt_ents:\n",
    "    # Unpack the tuple into start index, end index, and text\n",
    "    # Purpose: Extracts the components of each matched span tuple.\n",
    "    start, end, name = ent\n",
    "    \n",
    "    # Create a new Span object with the \"PERSON\" label\n",
    "    # Purpose: Assigns the matched span as a named entity with the label \"PERSON\".\n",
    "    per_ent = Span(doc, start, end, label=\"PERSON\")\n",
    "    \n",
    "    # Add the new Span to the list of entities\n",
    "    # Purpose: Collects the new \"PERSON\" entities for assignment to the Doc.\n",
    "    original_ents.append(per_ent)\n",
    "\n",
    "# Assign the updated list of entities to the Doc\n",
    "# Purpose: Sets the Doc’s entities to the newly created \"PERSON\" entities.\n",
    "doc.ents = original_ents\n",
    "\n",
    "\n",
    "# Print the Doc’s named entities\n",
    "# Purpose: Outputs the final list of named entities for inspection.\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e44d6",
   "metadata": {},
   "source": [
    "The code processes a text string using a blank spaCy English model to tokenize it, applies a regular expression pattern (e.g., r\"Paul [A-Z]\\w+\") to identify specific patterns like names, and converts these matches into spaCy Span objects labeled as \"PERSON\" entities. It stores these entities in a list and assigns them to the Doc’s ents attribute, effectively performing custom named entity recognition (NER) for patterns matching the regex. Finally, it prints the recognized entities. This is useful for extracting specific entities (e.g., names) from text when a pre-trained NER model is not available or suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "213200f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2, 'Paul Newman'), (8, 10, 'Paul Hollywood')]\n"
     ]
    }
   ],
   "source": [
    "print (mwt_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00680707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"paul_ner\")\n",
    "def paul_ner(doc):\n",
    "    pattern = r\"Paul [A-Z]\\w+\" \n",
    "    original_ents = list(doc.ents)\n",
    "    mwt_ents = []\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))\n",
    "    for ent in mwt_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"PERSON\")\n",
    "        original_ents.append(per_ent)\n",
    "    doc.ents = original_ents\n",
    "    return(doc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb5d55",
   "metadata": {},
   "source": [
    "The code defines a custom spaCy pipeline component named \"paul_ner\" using the @Language.component decorator. It processes a doc object by:\n",
    "\n",
    "Saving the original named entities from doc.ents into original_ents.\n",
    "Initializing an empty list mwt_ents to store matched spans.\n",
    "Using a regex pattern to find matches in the text, converting each match’s character span to a token-based Span, and adding valid spans (start index, end index, text) to mwt_ents.\n",
    "Converting each matched span into a new Span labeled as \"PERSON\" and appending it to original_ents.\n",
    "Updating doc.ents with the new list of entities.\n",
    "Returning the modified doc for use in the spaCy pipeline. This effectively adds custom entity recognition for patterns matching the regex (e.g., names like \"Paul Smith\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1822f096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.paul_ner(doc)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2 = spacy.blank(\"en\")\n",
    "nlp2.add_pipe(\"paul_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df22a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Paul Newman, Paul Hollywood)\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp2(text)\n",
    "print(doc2.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f09851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "@Language.component(\"cinema_ner\")\n",
    "def paul_ner(doc):\n",
    "    pattern = r\"Hollywood\" \n",
    "    original_ents = list(doc.ents)\n",
    "    mwt_ents = []\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))\n",
    "    for ent in mwt_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"CINEMA\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return(doc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40253f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.paul_ner(doc)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp3 = spacy.load(\"en_core_web_sm\")\n",
    "nlp3.add_pipe(\"cinema_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Newman PERSON\n",
      "American NORP\n",
      "Paul Hollywood PERSON\n",
      "British NORP\n",
      "Paul PERSON\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp3(text)\n",
    "for ent in doc3.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
